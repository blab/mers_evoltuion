Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job stats:
job                 count
----------------  -------
align                   1
all                     1
ancestral               1
export                  1
filter                  1
refine                  1
tidy_host_column        1
traits                  1
translate               1
tree                    1
total                  10

Select jobs to execute...

[Thu Jun 13 21:04:25 2024]
rule tidy_host_column:
    input: ../ingest/results/metadata.tsv
    output: results/metadata.tsv
    jobid: 6
    reason: Missing output files: results/metadata.tsv
    resources: tmpdir=/tmp

[Thu Jun 13 21:04:26 2024]
Finished job 6.
1 of 10 steps (10%) done
Select jobs to execute...

[Thu Jun 13 21:04:26 2024]
rule filter:
    input: ../ingest/results/sequences.fasta, results/metadata.tsv, defaults/dropped_strains.txt
    output: results/filtered.fasta
    jobid: 5
    reason: Missing output files: results/filtered.fasta; Input files updated by another job: results/metadata.tsv
    resources: tmpdir=/tmp

[Thu Jun 13 21:04:29 2024]
Error in rule filter:
    jobid: 5
    input: ../ingest/results/sequences.fasta, results/metadata.tsv, defaults/dropped_strains.txt
    output: results/filtered.fasta
    shell:
        
        augur filter             --sequences ../ingest/results/sequences.fasta             --metadata results/metadata.tsv             --exclude defaults/dropped_strains.txt             --exclude-where host != 'Camel'             --output results/filtered.fasta             --min-length 3000         
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-06-13T210425.650978.snakemake.log
